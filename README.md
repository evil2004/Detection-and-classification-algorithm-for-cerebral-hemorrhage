# 脑出血CT图像多标签分类项目

本项目使用深度学习方法对CT图像进行多标签分类，实现同时检测多种类型的脑出血（硬膜外出血、脑实质出血、脑室出血、蛛网膜下腔出血和硬膜下出血）。通过先进的数据处理技术和模型架构，本项目有效解决了医学影像分类中的不平衡数据问题。

## 目录

- [项目概述](#项目概述)
- [环境配置](#环境配置)
- [代码结构](#代码结构)
- [数据处理](#数据处理)
  - [数据格式](#数据格式)
  - [数据不平衡问题](#数据不平衡问题)
  - [平衡处理方法](#平衡处理方法)
  - [图像预处理](#图像预处理)
  - [数据增强](#数据增强)
- [模型架构](#模型架构)
  - [基础结构](#基础结构)
  - [损失函数](#损失函数)
  - [多阶段训练策略](#多阶段训练策略)
- [训练流程](#训练流程)
  - [数据准备](#数据准备-1)
  - [训练命令](#训练命令)
  - [训练过程监控](#训练过程监控)
- [评估结果](#评估结果)
  - [评估指标](#评估指标)
  - [最优阈值](#最优阈值)
  - [可视化](#可视化)
- [使用指南](#使用指南)
  - [模型预测](#模型预测)
  - [参数配置](#参数配置)
  - [注意事项](#注意事项)

## 项目概述

脑出血是一种严重的神经系统疾病，快速准确的诊断对于患者的预后至关重要。本项目通过深度学习技术，实现对CT图像中多种脑出血类型的自动识别，具有以下特点：

### 主要特点

1. **多标签分类**：
   - 同时识别5种脑出血类型（硬膜外出血、脑实质出血、脑室出血、蛛网膜下腔出血、硬膜下出血）
   - 使用Sigmoid激活函数进行多标签预测
   - 采用增强型Focal Loss处理类别不平衡问题

2. **先进的模型架构**：
   - Vision Transformer (ViT)作为骨干网络
   - 双向GRU序列模型捕捉切片间空间连续性
   - 自注意力机制聚合特征信息
   - 多阶段训练策略（分类头训练、部分微调、全面微调）

3. **数据平衡技术**：
   - KMeans-SMOTE生成合成样本
   - 欠采样+过采样组合策略
   - 合成样本加权处理

4. **多窗宽窗位组合**：
   - 脑窗(80, 40)
   - 软组织窗(200, 60)
   - 骨窗(1500, -600)

5. **自适应评估系统**：
   - 类别自适应阈值优化
   - 宏平均/微平均指标评估
   - 详细的可视化分析

## 环境配置

### 系统要求

- Python 3.7+
- CUDA支持（推荐）
- 8GB以上GPU显存

### 依赖安装

```bash
# 创建虚拟环境
python -m venv venv

# 激活虚拟环境
# Windows:
venv\\Scripts\\activate
# Linux/Mac:
source venv/bin/activate

# 安装依赖包
pip install -r requirements.txt
```

主要依赖包：

```
torch==1.12.0
torchvision==0.13.0
timm==0.6.12
pydicom==2.3.0
scikit-learn==1.1.2
matplotlib==3.5.3
pandas==1.4.3
numpy==1.23.2
opencv-python==4.6.0.66
imbalanced-learn==0.9.1
tqdm==4.64.0
```

## 代码结构

```
├── config.py            # 配置参数
├── dataset.py           # 数据集和数据增强实现
├── model.py             # 模型架构和损失函数定义
├── train.py             # 训练和验证流程
├── evaluate.py          # 模型评估
├── data_balance.py      # 数据平衡处理
├── utils.py             # 辅助工具函数
├── requirements.txt     # 依赖包列表
├── class_distribution.png # 类别分布可视化
└── README.md            # 项目说明
```

### 模型文件

- `best_model_initial.pth`：分类头训练的最佳模型
- `best_model_fine_tune.pth`：部分微调的最佳模型
- `best_model_full_tune.pth`：全面微调的最佳模型

### 结果目录

- `evaluation_results/`：评估结果和可视化
- `balanced_dataset/`：平衡处理后的数据集
- `smote_results/`：SMOTE生成的合成样本
- `training_results/`：训练过程记录

## 数据处理

### 数据格式

原始数据结构：
```
E:/shujuwajue/subset/
├── train_images/        # 训练集图像
├── test_images/         # 测试集图像
├── train_label.csv      # 训练集标签
└── test_label.csv       # 测试集标签
```

标签CSV格式：
```
filename,any,epidural,intraparenchymal,intraventricular,subarachnoid,subdural
ID_69e1264f2.dcm,1,0,1,0,0,0
...
```

### 数据不平衡问题

原始数据集中各类别出血样本分布不均，常见于医学影像数据。例如，硬膜外出血(epidural)和脑室出血(intraventricular)通常比其他类型的出血或无出血样本少得多。这种不平衡会导致模型偏向于预测多数类，而在少数类上表现不佳。

| 出血类型         | 大致占比 |
|-----------------|----------|
| epidural        | ~1-5%    |
| intraparenchymal| ~10-20%  |
| intraventricular| ~1-5%    |
| subarachnoid    | ~5-15%   |
| subdural        | ~5-15%   |
| 无出血(any=0)   | ~50-70%  |

*(注意：具体比例取决于数据集)*

### 平衡处理方法

为了解决数据不平衡问题，本项目采用了结合过采样和欠采样的方法：

1.  **KMeans-SMOTE合成**：
    *   **原理**: KMeans-SMOTE是SMOTE的一种改进变体。它首先使用KMeans算法将少数类样本聚类成 \(k\) 个簇，然后根据每个簇中少数类样本的数量或密度来确定该簇需要生成的合成样本数量。最后在每个簇内部独立应用SMOTE算法来生成合成样本。
    *   **优势**: 相比标准SMOTE，KMeans-SMOTE能更好地处理少数类内部可能存在的不同子分布或簇结构。通过聚类，它可以将噪声点分离到单独的簇中，减少在噪声点附近生成不当合成样本的可能性，有助于保持原始数据的局部结构。在医学影像中，同一病症的不同表现可能形成不同的特征簇，KMeans-SMOTE能更好地适应这种情况。

2.  **欠采样+过采样组合策略**：
    *   **策略**: 对样本量非常大的多数类（如无出血样本）进行随机欠采样，减少其数量。同时，对样本量较少的少数类（各种出血类型）应用KMeans-SMOTE进行过采样，增加其数量。
    *   **目标**: 使不同类别的样本数量达到一个相对平衡的状态，同时通过`target_ratio`和`max_ratio`参数控制，避免过度采样导致过拟合或改变数据原始分布过多。

3.  **强化数据增强**：
    *   **目的**: 进一步增加合成样本的多样性，提高模型的泛化能力。
    *   **方法**: 对通过KMeans-SMOTE生成的合成样本应用比原始真实样本更强的数据增强策略（例如更大的旋转角度、更强的颜色抖动等）。

### 图像预处理

CT图像的预处理对于模型性能至关重要：

1.  **DICOM处理**：
    *   直接从DICOM文件中读取像素数据。DICOM文件包含了图像信息以及患者、扫描参数等元数据（本项目主要使用像素数据）。

2.  **窗宽窗位 (Windowing) 设置与组合**:
    *   **背景**: CT图像的像素值（Hounsfield Units, HU）范围很广（通常-1000到+1000甚至更高），远超标准显示器能表现的灰度级（通常0-255）。不同的组织结构在不同的HU值范围内显示最佳。
    *   **窗宽(Window Width, WW)**: 定义了显示的HU值范围的宽度。
    *   **窗位(Window Level, WL)**: 定义了显示范围的中心HU值。
    *   **作用**: 通过选择合适的WW和WL，可以突出显示感兴趣的组织。本项目采用三种常用的窗口设置：
        *   **脑窗 (WW=80, WL=40)**: 优化脑实质（灰质、白质）以及早期出血（通常HU值略高于正常脑组织）的对比度。
        *   **软组织窗 (WW=200, WL=60)**: 更广泛地显示软组织结构，有助于观察颅骨附近或密度差异较大的区域。
        *   **骨窗 (WW=2800, WL=600)**: 主要用于清晰显示高密度的骨骼结构，虽然主要目标不是骨折，但可以提供出血位置与骨骼关系的参考。
    *   **组合优势**: 将这三种不同窗口设置下转换得到的灰度图像作为RGB图像的三个通道进行堆叠。这相当于为模型提供了三个不同对比度下的视图，融合了脑实质、软组织和骨骼结构的信息，使得模型可以学习到更丰富、更鲁棒的特征，从而提高分类的准确性。

3.  **尺寸调整与归一化**:
    *   将组合后的三通道图像调整为模型所需的输入尺寸（例如224x224）。
    *   根据预训练模型的均值和标准差对图像进行归一化。

### 数据增强

数据增强是提高模型泛化能力、减少过拟合的关键技术：

1.  **基础增强** (应用于所有训练样本)：
    *   随机水平翻转 (p=0.5)
    *   随机旋转 (±10度)
    *   随机仿射变换 (包括轻微平移±5%, 缩放±5%)

2.  **强化增强** (额外应用于合成样本)：
    *   随机水平翻转 (p=0.7，概率更高)
    *   随机垂直翻转 (p=0.3，一般不用于自然图像，但在特定角度CT上可能适用)
    *   随机旋转 (±15度，范围更大)
    *   随机仿射变换 (平移±10%, 缩放±10%，范围更大)

3.  **颜色/强度抖动**:
    *   亮度、对比度、饱和度、色调的随机调整。对合成样本应用更强的抖动范围，进一步增加其多样性。

## 模型架构

### 基础结构

本项目采用基于Vision Transformer (ViT)的深度学习模型，并结合序列处理模块，以适应CT图像的特性：

1.  **特征提取器 (ViT)**:
    *   **模型**: ViT-Base (ViT-B/16)，即基础版本的ViT，输入图像被分割成16x16像素的块。
    *   **工作原理**: ViT将图像块（Patches）视为序列数据（类似NLP中的词元Tokens），通过线性嵌入将其转换为向量，并加入位置编码信息。这些向量序列随后被送入标准的Transformer编码器（由多层自注意力机制和前馈网络组成）。
    *   **优势**: ViT能够通过自注意力机制捕捉图像中长距离的依赖关系，获取全局上下文信息。本项目利用了在大型图像数据集（如ImageNet）上预训练的ViT权重，进行迁移学习，可以显著加速训练并提升模型性能。
    *   **实现**: 使用`timm`库加载预训练的ViT模型，并移除其原始的分类头。

2.  **2D分类头**:
    *   **作用**: 对单个CT切片（经过ViT提取特征后）进行分类预测。
    *   **结构**: 一个简单的多层全连接网络（MLP），接收ViT输出的特征向量（维度通常为768），经过几层线性变换、激活函数（如ReLU）、批量归一化（BatchNorm）和Dropout（防止过拟合），最终输出对应5种出血类型的概率（经过Sigmoid激活）。

3.  **序列处理模块 (BiGRU)** (可选，通过`--use_sequence`启用):
    *   **目的**: 捕捉CT切片之间的空间连续性信息。脑出血往往会影响相邻的多个切片。
    *   **结构**: 双向门控循环单元（Bidirectional Gated Recurrent Unit）。GRU是RNN的一种变体，能有效处理序列依赖。双向结构使其能同时利用当前切片之前和之后的信息。
    *   **输入**: 将一个CT病例的多个连续切片（例如24个）分别通过ViT提取特征，得到特征向量序列 `[batch_size, sequence_length, feature_dim]`。
    *   **输出**: BiGRU处理该序列，输出融合了上下文信息的新的特征序列 `[batch_size, sequence_length, hidden_dim*2]`。

4.  **自注意力层 (Self-Attention)** (在BiGRU之后):
    *   **目的**: 对BiGRU输出的序列特征进行加权聚合。并非序列中的所有切片都对最终诊断同等重要。
    *   **机制**: 计算序列中不同时间步（切片）特征之间的相关性，为每个时间步分配注意力权重。
    *   **输出**: 将序列特征根据注意力权重加权求和，得到一个固定长度的、代表整个序列关键信息的向量。

5.  **集成分类头 (Ensemble Classifier)** (当使用序列模型时):
    *   **作用**: 融合来自不同信息源的特征进行最终预测。
    *   **输入**: 将ViT提取的（可能来自中心切片的）原始特征、2D分类头的预测结果（可选）、以及经过BiGRU和自注意力处理后的序列聚合特征拼接起来。
    *   **输出**: 通过另一个MLP分类头，输出最终的多标签预测概率。

### 损失函数

采用**增强型Focal Loss**来解决多标签分类中的类别不平衡问题：

```python
class EnhancedFocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, synthetic_weight=None, class_weights=None):
        # ... 实现 ...
```

*   **Focal Loss核心思想**: 降低容易分类的样本（通常是多数类的负样本）对损失函数的贡献，让模型更专注于学习困难样本（通常是少数类的正样本和困难的负样本）。通过调制因子 \((1-p_t)^\gamma\) 实现，其中 \(p_t\) 是模型预测正确类别的概率，\(\gamma\) (gamma)是聚焦参数（\(\gamma > 0\)，通常取2）。当样本被正确分类且置信度高时，\(p_t\) 接近1，调制因子接近0，损失贡献小；反之，损失贡献大。
*   **增强**:
    *   `alpha`: 可以为不同类别设置不同的权重，进一步平衡类别重要性。
    *   `synthetic_weight`: 可以为SMOTE等方法生成的合成样本设置不同的权重，例如降低合成样本的权重以减少其可能引入的噪声影响。
    *   `class_weights`: 显式地为每个类别设置权重。

### 多阶段训练策略

为了有效利用预训练模型并避免破坏其学到的知识，采用分阶段微调（Fine-tuning）策略：

1.  **初始阶段 (Initial Training)**:
    *   **操作**: 冻结ViT主干网络的所有参数，只训练新添加的分类头（2D分类头，如果使用序列模型，也包括序列模块和集成分类头）。
    *   **目的**: 让模型的"决策部分"快速适应脑出血分类任务的特征映射，而不影响ViT的通用特征提取能力。
    *   **学习率**: 相对较大（例如 `Config.LEARNING_RATE = 1e-4`），以加速收敛。
    *   **轮数**: 通常15-20轮，或直到验证损失稳定。

2.  **部分微调 (Partial Fine-tuning)**:
    *   **操作**: 保持ViT的早期层冻结，解冻靠近输出的部分层（例如最后几个Transformer块）。继续训练解冻的ViT层和分类头。
    *   **目的**: 让模型的高层特征表示更适应当前任务，同时保留底层通用特征。
    *   **学习率**: 较小（例如 `5e-5`），以进行精细调整，避免大幅改变已学到的权重。
    *   **轮数**: 通常10-15轮。

3.  **全面微调 (Full Fine-tuning)**:
    *   **操作**: 解冻ViT的所有层，让整个网络都参与训练。
    *   **目的**: 对整个模型进行端到端的微调，以期达到最佳性能。
    *   **学习率**: 非常小（例如 `1e-5`），以避免破坏预训练权重，仅做微小调整。
    *   **轮数**: 通常5-10轮，或使用早停机制。

**原理**: 这种渐进式解冻和逐步降低学习率的策略，是一种常见的迁移学习技巧。它旨在平稳地将预训练模型的能力迁移到新的任务和数据集上，最大限度地保留预训练知识，同时适应新数据的特性，从而获得比直接从头训练或一次性完全微调更好的效果，并有助于提高训练的稳定性。

## 训练流程

### 数据准备

1.  **数据平衡处理** (可选但推荐):
    *   运行 `data_balance.py` 脚本对原始训练集CSV文件进行处理，生成包含合成样本的平衡后的CSV文件。
    ```bash
    python data_balance.py --input_csv <原始训练集CSV路径> --output_csv <平衡后CSV保存路径> [--target_ratio 0.1] [--max_ratio 0.2]
    ```
    *   此步骤会读取原始图像文件以提取特征用于SMOTE，可能需要一定时间。

2.  **训练验证集划分**:
    *   在 `train.py` 中，使用 `get_train_val_dataloaders` 函数自动将（平衡后的）训练数据划分为训练集和验证集（默认比例80%/20%）。划分时会考虑类别分布，尽量保持验证集与训练集分布相似。

### 训练命令

根据需要选择不同的训练模式：

```bash
# 完整训练流程（执行上述三个阶段）
# 需要提供平衡后的训练CSV和图像目录
python train.py --train_csv <平衡后训练集CSV> --train_dir <训练图像目录> --multi_stage

# 只执行单个训练阶段
# 例如，只进行初始训练
python train.py --train_csv <平衡后训练集CSV> --train_dir <训练图像目录> --stage initial --epochs 20
# 例如，加载初始训练模型，进行部分微调
python train.py --train_csv <平衡后训练集CSV> --train_dir <训练图像目录> --stage fine_tune --epochs 15 --load_path best_model_initial.pth
# 例如，加载部分微调模型，进行全面微调
python train.py --train_csv <平衡后训练集CSV> --train_dir <训练图像目录> --stage full_tune --epochs 10 --load_path best_model_fine_tune.pth

# 启用序列模型进行训练 (与--multi_stage或--stage结合使用)
python train.py --train_csv <平衡后训练集CSV> --train_dir <训练图像目录> --multi_stage --use_sequence
```

### 训练过程监控

训练脚本内置了多种监控和调整机制：

1.  **早停机制 (Early Stopping)**:
    *   **作用**: 防止模型在训练集上过拟合，即训练损失持续下降但验证损失开始上升。
    *   **机制**: 监控验证集损失 (`val_loss`)。如果在 `Config.EARLY_STOPPING_PATIENCE`（例如7）个轮次内，验证损失没有创下新低，则自动停止训练。通常会保存验证损失最低时的模型状态。

2.  **学习率调整 (Learning Rate Scheduling)**:
    *   **初始阶段**: 使用 `ReduceLROnPlateau`。当验证损失在 `Config.SCHEDULER_PATIENCE` 轮内没有改善时，将学习率乘以 `Config.SCHEDULER_FACTOR`（例如0.1）。
    *   **微调阶段**: 使用 `CosineAnnealingWarmRestarts`。学习率按余弦曲线周期性地在初始值和最小值之间变化，并在每个周期开始时"重启"。这有助于模型跳出局部最优。

3.  **进度条与指标**:
    *   使用 `tqdm` 显示每个epoch的训练和验证进度。
    *   实时显示当前批次的平均损失(loss)、宏平均精确率(prec)、宏平均召回率(rec)和宏平均F1分数(f1)。

4.  **结果保存**:
    *   每个epoch结束后，记录训练损失和验证损失。
    *   训练结束后，保存训练/验证损失曲线图 (`training_curve.png`)。
    *   验证集上性能最佳的模型权重会保存到对应的文件（如 `best_model_initial.pth`, `best_model_fine_tune.pth`, `best_model_full_tune.pth`）。
    *   保存包含训练配置和最终指标的JSON文件 (`training_summary.json`)。

## 评估结果

训练完成后，使用 `evaluate.py` 脚本在测试集上评估模型的性能。

### 评估指标

对每个出血类别以及整体性能计算以下指标：

1.  **精确率 (Precision)**: 预测为该类别的样本中，实际是该类别的比例。 \( P = \frac{TP}{TP+FP} \)
2.  **召回率 (Recall)**: 实际为该类别的样本中，被正确预测出来的比例。 \( R = \frac{TP}{TP+FN} \)
3.  **F1分数 (F1-Score)**: 精确率和召回率的调和平均值，综合反映模型的性能。 \( F1 = 2 \times \frac{P \times R}{P + R} \)
4.  **AUC (Area Under the ROC Curve)**: ROC曲线下的面积，衡量模型区分正负样本的能力，对阈值不敏感。值越接近1越好。
5.  **混淆矩阵 (Confusion Matrix)**: 可视化每个类别的真阳性(TP)、假阳性(FP)、真阴性(TN)、假阴性(FN)数量。

同时计算这些指标的**宏平均 (Macro Average)** 和 **微平均 (Micro Average)**：
*   **宏平均**: 对每个类别的指标独立计算后取平均，平等对待所有类别。更能反映在稀有类别上的表现。
*   **微平均**: 将所有类别的TP/FP/FN累加后计算全局指标，受样本量大的类别影响更大。

### 最优阈值

由于模型输出的是概率值，需要选择一个阈值来将概率转换为最终的类别预测（出血/无出血）。默认阈值为0.5，但这不一定是最优的。`evaluate.py` 提供了寻找最优阈值的功能：

```bash
python evaluate.py --model_path <模型路径> --test_csv <测试集CSV> --test_dir <测试图像目录> --optimize_thresholds
```

*   该功能会针对每个类别，在验证集（或测试集，取决于实现）上尝试不同的阈值（例如从0.1到0.9），并选择使得该类别F1分数最高的阈值作为最优阈值。
*   找到的最优阈值可以用于后续的预测，以获得更好的平衡性能。评估脚本会先找到最优阈值，然后使用这些阈值重新计算并报告最终的测试集性能指标。

### 可视化

评估脚本会生成以下可视化结果，保存在 `evaluation_results/` 目录下：

1.  **混淆矩阵 (`confusion_matrices.png`)**: 为每个出血类别绘制一个2x2的混淆矩阵图。
2.  **ROC曲线 (`roc_curves.png`)**: 在同一张图上绘制每个出血类别的ROC曲线，并标注AUC值。
3.  **(训练时生成) 类别分布 (`class_distribution.png`)**: 可视化展示原始数据集和（如果使用了）平衡后数据集中各类别的样本数量或比例。

## 使用指南

### 模型预测

使用训练好的模型对新的测试数据进行预测：

```bash
# 使用默认0.5阈值进行评估和预测
python evaluate.py --model_path <模型路径> --test_csv <测试集CSV> --test_dir <测试图像目录>

# 使用优化后的阈值进行评估，并保存预测结果到CSV文件
python evaluate.py --model_path <模型路径> --test_csv <测试集CSV> --test_dir <测试图像目录> --optimize_thresholds --save_predictions --output_dir ./evaluation_results
```
预测结果将保存在指定的 `output_dir` 下的 `predictions.csv` 文件中。

### 参数配置

项目的主要配置参数定义在 `config.py` 文件中，可以根据需要进行修改：

1.  **数据路径**:
    ```python
    TRAIN_CSV = "path/to/your/train_label.csv"
    TEST_CSV = "path/to/your/test_label.csv"
    TRAIN_IMAGES_DIR = "path/to/your/train_images/"
    TEST_IMAGES_DIR = "path/to/your/test_images/"
    BALANCED_TRAIN_CSV = "balanced_dataset/balanced_train_label.csv" # 平衡后的CSV路径
    ```

2.  **模型参数**:
    ```python
    MODEL_NAME = "vit_base_patch16_224"  # 使用的ViT模型
    IMAGE_SIZE = 224                      # 输入图像尺寸
    NUM_CLASSES = 5                       # 出血类型数量
    SEQUENCE_LENGTH = 24                  # 序列模型使用的切片数量 (如果启用)
    ```

3.  **训练参数**:
    ```python
    BATCH_SIZE = 32                       # 批次大小 (根据GPU显存调整)
    EPOCHS = 50                           # 总训练轮数 (多阶段训练时会被覆盖)
    LEARNING_RATE = 1e-4                  # 初始学习率
    WEIGHT_DECAY = 1e-5                   # 权重衰减 (L2正则化)
    EARLY_STOPPING_PATIENCE = 7           # 早停耐心轮数
    SCHEDULER_PATIENCE = 3                # 学习率调度器耐心轮数
    SCHEDULER_FACTOR = 0.1                # 学习率衰减因子
    ```

4.  **窗宽窗位设置**:
```python
WINDOW_SETTINGS = [
    (80, 40),     # 脑窗
    (200, 60),    # 软组织窗
    (1500, -600)  # 骨窗
]
    NORMALIZE_MEAN = [0.485, 0.456, 0.406] # 图像归一化均值 (通常使用ImageNet预训练值)
    NORMALIZE_STD = [0.229, 0.224, 0.225]  # 图像归一化标准差
    ```

5.  **数据增强参数**:
    ```python
    BRIGHTNESS = 0.2
    CONTRAST = 0.2
    SATURATION = 0.2
    HUE = 0.1
    ```

6.  **数据平衡参数**:
    ```python
    SYNTHETIC_SAMPLE_WEIGHT = 0.8         # Focal Loss中合成样本的权重 (小于1表示降低权重)
    # data_balance.py脚本参数
    TARGET_RATIO = 0.1                    # KMeans-SMOTE的目标比例
    MAX_RATIO = 0.2                       # 单个类别允许的最大比例
    ```
